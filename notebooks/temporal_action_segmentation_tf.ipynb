{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Temporal Action Segmentation in Videos Using Recurrent Neural Networks\n",
    "\n",
    "Welcome! In this lab, you'll learn how to train a recurrent neural network on the [Breakfast dataset](http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/) for the task of temporal action segmentation and recognition. Our goal is to predict one action label for each frame of a long untrimmed video showing a person preparing breakfast. We will go through all the steps, including loading the data, building and training a model, calculating the accuracy, and making predictions. We will use the [Tensorflow library](https://github.com/tensorflow/tensorflow) to build and train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakfast Dataset Overview\n",
    "\n",
    "[Breakfast dataset](http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/) is a video dataset containing 1712 long untrimmed videos, each one containing a sequence of multiple actions. Videos contain actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens. It features videos from multiple cameras, which were uncalibrated and their position changes based on the location. There are ∼77 hours of video (> 4 million frames). The resolution of videos is 320×240 pixels and the frame rate is 15 fps.\n",
    "\n",
    "For this lab, we will use precomputed features describing each frame of the video and our goal is to predict a label for the action performed by the person in each frame of a video.\n",
    "![Example output](http://serre-lab.clps.brown.edu/wp-content/uploads/2012/04/juice_frame550.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing software and importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import logging\n",
    "from numpy import array, vstack, sum, prod, argmax\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple, defaultdict\n",
    "import pdb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import clip_ops, array_ops, nn_ops, math_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.contrib.rnn import LSTMCell, GRUCell, MultiRNNCell\n",
    "\n",
    "from datasets.breakfast import BreakfastDataset\n",
    "from datasets.batch_generator import FrameSequenceBatchGenerator\n",
    "from utils.preprocessing import pad_sequences, DataPreprocessor\n",
    "from utils.metrics import per_frame_accuracy\n",
    "from utils.tf_utils import debug_nans, to_categorical, clip_gradients, \\\n",
    "    stack_bidirectional_dynamic_rnn\n",
    "from utils.plot_utils import plot_optimization_log_frame\n",
    "from utils.my_io_utils import save_to_pickle\n",
    "from utils.misc import frame_labels_to_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: download data with command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_annotations_json_file = '../data/breakfast/annotations_train_split0.json'\n",
    "testing_annotations_json_file = '../data/breakfast/annotations_test_split0.json'\n",
    "downsampling_factor = 15\n",
    "\n",
    "# BreakfastDataset class implements methods: __len__(): returning the number of training/testing samples/videos\n",
    "# and __getitem__: returning a dictionary with keys: 'video_name', 'feat', 'labels', 'frame_indices'\n",
    "training_set = BreakfastDataset(dataset_path='../data/breakfast', downsampling_factor=downsampling_factor,\n",
    "                 annotations_json_file=training_annotations_json_file)\n",
    "\n",
    "testing_set = BreakfastDataset(dataset_path='../data/breakfast', downsampling_factor=downsampling_factor,\n",
    "                 annotations_json_file=testing_annotations_json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look into one training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_training_samples = len(training_set)\n",
    "print('Nb training videos: {:d}'.format(nb_training_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sample_ind = 0\n",
    "training_sample = training_set[training_sample_ind]\n",
    "segs, seg_labels = frame_labels_to_segments(training_sample['labels'])\n",
    "action_names = training_set.get_action_names()\n",
    "action_names_sequence = [action_names[seg_label] for seg_label in seg_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training sample {} video name: {}'.format(training_sample_ind, training_sample['video_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training sample {} feature shape (nb_frames, feat_dim): {}'.format(training_sample_ind, str(training_sample['feat'].shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training sample {} per frame labels: {}'.format(training_sample_ind, str(training_sample['labels'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training sample {} sequence of actions : {}'.format(training_sample_ind, str(action_names_sequence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training sample {} frame indices: {}'.format(training_sample_ind, str(training_sample['frame_indices'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(action_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Preprocessing data\n",
    "\n",
    "Preprocessing includes padding feature sequences, so that they all have the same length, and converting labels from an integer format (e.g., \"2\"), to an [one hot encoding](https://en.wikipedia.org/wiki/One-hot) (e.g., \"0, 0, 1, 0, 0, 0, 0, 0, 0, 0\"). We will define a class that implements a preprocess method with arguments features_lst and frame_labels_lst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDataPreprocessor(DataPreprocessor):\n",
    "    def __init__(self, preprocessor_params):\n",
    "        \"\"\"\n",
    "        :param preprocessor_params: dict with keys:\n",
    "                       'encoder_nb_classes' and 'max_nb_frames',\n",
    "                       encoder_nb_classes: number of classes for encoder\n",
    "                                          (number of action labels)\n",
    "                       max_nb_frames: maximum number of frames\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(preprocessor_params)\n",
    "\n",
    "    def preprocess(self, features_lst, frame_labels_lst):\n",
    "        \"\"\"\n",
    "        Pads feature sequences and frame_labels to max_nb_frames\n",
    "        Converts frame_labels to one-hot encoding\n",
    "\n",
    "        :param features_lst: list with nb_samples elements. Each element\n",
    "                              is a numpy array of size (timesteps, feat_dim)\n",
    "        :param frame_labels_lst: a list with nb_samples elements.\n",
    "                    Each element is a (nb_timesteps,) numpy array or list,\n",
    "                    each element of which list is a list of the labels for\n",
    "                    this timestep\n",
    "        :return: feat: numpy array with shape (nb_samples, max_nb_frames,\n",
    "                       feat_dim)\n",
    "                 frame_labels: numpy array with shape\n",
    "                              (nb_samples, max_nb_frames, encoder_nb_classes)\n",
    "                 frame_sample_weights: numpy binary array with shape\n",
    "                   (nb_samples, max_nb_frames, 1),\n",
    "                   having zeroes at padded indices and ones elsewhere\n",
    "                 frame_sequence_lengths: numpy array with shape = (nb_samples, )\n",
    "                   with true length (nb_timesteps) per sample sequence\n",
    "        \"\"\"\n",
    "        encoder_nb_classes = self.params['encoder_nb_classes']\n",
    "        max_nb_frames = self.params['max_nb_frames']\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # Process features and frame_labels (Pad and one-hot encoding)\n",
    "        frame_sequence_lengths = [x.shape[0] for x in features_lst]\n",
    "        feat, frame_sample_weights = pad_sequences(\n",
    "            features_lst, max_len=max_nb_frames,\n",
    "            dtype='float32', value=0.)\n",
    "\n",
    "        # Format labels\n",
    "        # Go from y_t = {1...C} to one-hot vector (e.g. y_t = [1, 0, 1, 0])\n",
    "        # list of arrays (timesteps, num_classes))\n",
    "        frame_labels_lst = [to_categorical(y, encoder_nb_classes)\n",
    "                            for y in frame_labels_lst]\n",
    "\n",
    "        # Pad frame labels sequences\n",
    "        frame_labels, _ = pad_sequences(frame_labels_lst, max_len=max_nb_frames,\n",
    "                                        dtype='float32', value=0.)\n",
    "\n",
    "        return feat, frame_labels, frame_sample_weights, \\\n",
    "            frame_sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(action_names)\n",
    "preprocessor_obj = RNNDataPreprocessor(\n",
    "    preprocessor_params={\n",
    "        'encoder_nb_classes': nb_classes,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Define data generators, providing batches of data\n",
    "\n",
    "We will now get training and testing data generators, implementing the method __next__(), which is yielding batches of data. Each batch contains features: (batch_size, nb_timesteps, feat_dim),  frame_labels: (batch_size, nb_timesteps, nb_classes), frame_sample_weights: (batch_size, nb_timesteps,) and frame_sequence_lengths: (batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "training_batch_generator_obj = FrameSequenceBatchGenerator(batch_size, dataset_obj=training_set, preprocessor_obj=preprocessor_obj, nb_classes=nb_classes,\n",
    "                 shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testing_batch_generator_obj = FrameSequenceBatchGenerator(batch_size, dataset_obj=testing_set, preprocessor_obj=preprocessor_obj, nb_classes=nb_classes,\n",
    "                 shuffle=False, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nb_frames_train = training_batch_generator_obj.get_max_nb_frames()\n",
    "print('Maximum number of frames in training videos: {:d}'.format(max_nb_frames_train))\n",
    "max_nb_frames_test = testing_batch_generator_obj.get_max_nb_frames()\n",
    "print('Maximum number of frames in testing videos: {:d}'.format(max_nb_frames_test))\n",
    "max_nb_frames = max(max_nb_frames_train, max_nb_frames_test)\n",
    "training_batch_generator_obj.configure_preprocessing(_max_nb_frames=max_nb_frames)\n",
    "testing_batch_generator_obj.configure_preprocessing(_max_nb_frames=max_nb_frames)\n",
    "\n",
    "feat_dim = training_batch_generator_obj.get_feat_dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Build tensorflow model\n",
    "\n",
    "We will now declare our Recurrent Neural Network model. TensorFlow uses a dataflow graph to represent our computation in terms of the dependencies between individual operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4a) First, we will set the values of our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = 0\n",
    "\n",
    "# Output returned after running a forward pass through our model.\n",
    "InferenceOutput = namedtuple(\n",
    "    \"InferenceOutput\",\n",
    "    \"frame_logits frame_predictions frame_y_pred\")\n",
    "\n",
    "# Results on validation set\n",
    "ValFrameResults = namedtuple(\n",
    "    \"ValFrameLevelResults\",\n",
    "    \"val_frame_metric val_frame_loss val_frame_predictions val_frame_y_pred \"\n",
    "    \"val_frame_y_true val_frame_sample_weights \"\n",
    "    \"val_frame_logits\"\n",
    ")\n",
    "\n",
    "# Results on training set\n",
    "TrainFrameResults = namedtuple(\n",
    "    \"TrainFrameLevelResults\",\n",
    "    \"train_frame_metric train_frame_loss train_frame_predictions \"\n",
    "    \"train_frame_y_pred train_frame_y_true train_frame_sample_weights \"\n",
    "    \"train_frame_logits\"\n",
    ")\n",
    "\n",
    "# Output of rnn encoder\n",
    "EncoderOutput = namedtuple(\n",
    "    \"EncoderOutput\",\n",
    "    \"outputs final_state\")\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"feat_dim\": feat_dim,\n",
    "    \"encoder_nb_classes\": nb_classes,\n",
    "    \"max_nb_frames\": max_nb_frames,\n",
    "    # Model parameters\n",
    "    \"encoder_nb_hidden_units\": 256,\n",
    "    \"encoder_cell_type\": 'gru',\n",
    "    \"encoder_is_bidirectional\": True,\n",
    "    \"encoder_activation\": 'tanh',\n",
    "    \"encoder_initializer\": 'random_uniform',\n",
    "    \"encoder_init_scale\": 0.1,\n",
    "    \"encoder_clip_gradients\": 1,\n",
    "    \"encoder_nb_layers\": 2,\n",
    "    # Training/Optimizer parameters\n",
    "    \"dropout_rate\": 0.3,\n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"nb_epochs\": 50,\n",
    "    \"shuffle\": True,\n",
    "    \"momentum\": 0.0,\n",
    "    \"decay_period\": 10.0,\n",
    "    \"decay_rate\": 0.5,\n",
    "    \"nesterov\": False,\n",
    "    \"seed\": 42,\n",
    "    \"log_dir\": '../data/breakfast/results',\n",
    "}\n",
    "\n",
    "if params['encoder_initializer'] == 'None':\n",
    "    params['encoder_initializer'] = None\n",
    "if params['encoder_clip_gradients'] == -1:\n",
    "    params['encoder_clip_gradients'] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4b) Then, we will define our network architecture, i.e. all the operations\n",
    "required to go from an input sequence of features to an output sequence of class predictions per frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_fc_layer(layer_inputs, initializer):\n",
    "    # Output Fully Connected layer\n",
    "    layer_name = \"encoder_time_fully_connected\"\n",
    "    with tf.name_scope(layer_name):\n",
    "        # Apply dense layer to each timestep\n",
    "        # kernel: (nb_hidden_states , nb_classes)\n",
    "        # bias: (nb_classes,)\n",
    "\n",
    "        unstacked = tf.unstack(layer_inputs, axis=1)\n",
    "        dense_res = [tf.layers.dense(\n",
    "            inputs=s, units=params['encoder_nb_classes'],\n",
    "            activation=None,\n",
    "            use_bias=True, kernel_initializer=initializer,\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=None, bias_regularizer=None,\n",
    "            activity_regularizer=None, trainable=True,\n",
    "            name=layer_name, reuse=(i != 0))\n",
    "            for (i, s) in enumerate(unstacked)]\n",
    "\n",
    "        # output: 3D Tensor (batch_size, nb_timesteps,\n",
    "        # nb_classes)\n",
    "        frame_logits = tf.stack(dense_res, axis=1)\n",
    "        frame_logits = debug_nans(frame_logits, \"frame_logits\", debug=DEBUG)\n",
    "        print(\"Logits static shape: \",\n",
    "              frame_logits.shape)\n",
    "        frame_predictions = tf.nn.softmax(frame_logits)\n",
    "        frame_y_pred = tf.cast(tf.argmax(frame_logits, axis=-1), tf.int32)\n",
    "\n",
    "        # Create summaries for tensorboard visualization\n",
    "        fc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                    layer_name)\n",
    "        tf.summary.histogram('kernel', fc_vars[0])\n",
    "        tf.summary.histogram('bias', fc_vars[1])\n",
    "        tf.summary.histogram('frame_logits', frame_logits)\n",
    "        tf.summary.histogram('frame_predictions', frame_predictions)\n",
    "\n",
    "        return frame_logits, frame_predictions, frame_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_cell(rnn_cell_params, is_training, layer_name='rnn_cell'):\n",
    "    with tf.variable_scope(layer_name):\n",
    "\n",
    "        # Initialize cell weights/biases\n",
    "        if rnn_cell_params['initializer'] is None:\n",
    "            initializer = None\n",
    "        elif rnn_cell_params['initializer'] == \"random_uniform\":\n",
    "            init_scale = rnn_cell_params['init_scale'] / rnn_cell_params[\n",
    "                'nb_hidden_units']\n",
    "            initializer = tf.random_uniform_initializer(\n",
    "                -init_scale, init_scale)\n",
    "        elif rnn_cell_params['initializer'] == 'random_normal':\n",
    "            initializer = tf.random_normal_initializer(\n",
    "                0, rnn_cell_params['init_scale'])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid initializer %s\",\n",
    "                             rnn_cell_params['initializer'])\n",
    "        \n",
    "        # Get forward and backward cells for bidirectional RNN. Weights are not shared.\n",
    "        if rnn_cell_params['cell_type'] == 'lstm':\n",
    "            # LSTM Cell\n",
    "            # Default activation: tanh\n",
    "            if rnn_cell_params['activation'] == 'tanh':\n",
    "                activation = tf.nn.tanh\n",
    "            else:\n",
    "                activation = None\n",
    "\n",
    "            cell_fw = LSTMCell(num_units=rnn_cell_params['nb_hidden_units'],\n",
    "                               use_peepholes=False,\n",
    "                               cell_clip=None, initializer=initializer,\n",
    "                               num_proj=None, proj_clip=None,\n",
    "                               forget_bias=1.0, state_is_tuple=True,\n",
    "                               activation=activation)\n",
    "            cell_bw = LSTMCell(num_units=rnn_cell_params['nb_hidden_units'],\n",
    "                               use_peepholes=False,\n",
    "                               cell_clip=None, initializer=initializer,\n",
    "                               num_proj=None, proj_clip=None,\n",
    "                               forget_bias=1.0, state_is_tuple=True,\n",
    "                               activation=activation)\n",
    "        elif rnn_cell_params['cell_type'] == 'gru':\n",
    "            # GRU Cell\n",
    "            # If bias_initializer is None, then it starts with bias\n",
    "            # of 1.0\n",
    "            # to not reset and not update.\n",
    "            # Default activation: tanh\n",
    "            if rnn_cell_params['activation'] == 'tanh':\n",
    "                activation = tf.nn.tanh\n",
    "            else:\n",
    "                activation = None\n",
    "            cell_fw = GRUCell(num_units=rnn_cell_params['nb_hidden_units'],\n",
    "                              activation=activation,\n",
    "                              reuse=None,\n",
    "                              kernel_initializer=initializer,\n",
    "                              bias_initializer=None)\n",
    "            cell_bw = GRUCell(num_units=rnn_cell_params['nb_hidden_units'],\n",
    "                              activation=activation,\n",
    "                              reuse=None,\n",
    "                              kernel_initializer=initializer,\n",
    "                              bias_initializer=None)\n",
    "        else:\n",
    "            raise ValueError('Not supported cell type: %s',\n",
    "                             rnn_cell_params['cell_type'])\n",
    "\n",
    "        # Add dropout\n",
    "        output_keep_prob = 1 - rnn_cell_params['dropout_rate']\n",
    "        dropout_cell_fw = tf.contrib.rnn.DropoutWrapper(\n",
    "            cell_fw, input_keep_prob=1.0,\n",
    "            output_keep_prob=tf.cond(\n",
    "                is_training,\n",
    "                lambda: tf.constant(output_keep_prob),\n",
    "                lambda: tf.constant(1.0)),\n",
    "            state_keep_prob=1.0,\n",
    "            variational_recurrent=True, input_size=None,\n",
    "            dtype=tf.float32, seed=None)\n",
    "        dropout_cell_bw = tf.contrib.rnn.DropoutWrapper(\n",
    "            cell_bw, input_keep_prob=1.0,\n",
    "            output_keep_prob=tf.cond(\n",
    "                is_training,\n",
    "                lambda: tf.constant(output_keep_prob),\n",
    "                lambda: tf.constant(1.0)),\n",
    "            state_keep_prob=1.0,\n",
    "            variational_recurrent=True, input_size=None,\n",
    "            dtype=tf.float32, seed=None)\n",
    "\n",
    "        return dropout_cell_fw, dropout_cell_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_encoder(input_sequences, is_training, sequence_lengths, encoder_params):\n",
    "    # layer_inputs: [batch_size, max_len, feat_dim]\n",
    "    layer_inputs = input_sequences\n",
    "    layer_inputs = debug_nans(layer_inputs, \"layer_inputs\", debug=DEBUG)\n",
    "    print(\"Layer inputs static shape: \", layer_inputs.shape)\n",
    "    layer_name = \"encoder_rnn\"\n",
    "\n",
    "    # Get forward/backward cells for each layer\n",
    "    cells_fw = []\n",
    "    cells_bw = []\n",
    "    for _ in range(encoder_params['nb_layers']):\n",
    "        cell_fw, cell_bw = get_rnn_cell(encoder_params, is_training=is_training,\n",
    "                                        layer_name=layer_name)\n",
    "        cells_fw.append(cell_fw)\n",
    "        cells_bw.append(cell_bw)\n",
    "\n",
    "    if encoder_params['is_bidirectional']:\n",
    "        # outputs: concatenated fw and bw hidden states of last layer\n",
    "        # output_state_fw, output_state_bw = final_state\n",
    "        # output_states_fw is the final states, one tensor per layer,\n",
    "        # of the forward rnn.\n",
    "        # output_states_bw is the final states, one tensor per layer,\n",
    "        # of the backward rnn.\n",
    "\n",
    "        outputs, output_states_fw, output_states_bw, _ = \\\n",
    "            stack_bidirectional_dynamic_rnn(\n",
    "                cells_fw,\n",
    "                cells_bw,\n",
    "                inputs=layer_inputs,\n",
    "                initial_states_fw=None,\n",
    "                initial_states_bw=None,\n",
    "                dtype=tf.float32,\n",
    "                sequence_length=sequence_lengths,\n",
    "                parallel_iterations=None,\n",
    "                scope=layer_name)\n",
    "        final_state = (output_states_fw, output_states_bw)\n",
    "    else:\n",
    "        multi_rnn_cell = MultiRNNCell(cells_fw, state_is_tuple=True)\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(\n",
    "            cell=multi_rnn_cell, inputs=layer_inputs,\n",
    "            sequence_length=sequence_lengths, initial_state=None,\n",
    "            dtype=tf.float32, parallel_iterations=None,\n",
    "            time_major=False, scope=layer_name)\n",
    "\n",
    "    return EncoderOutput(\n",
    "        outputs=outputs,\n",
    "        final_state=final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inference(input_sequences, is_training,\n",
    "               sequence_lengths=None):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_sequences:\n",
    "    :param is_training:\n",
    "    :param sequence_lengths:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    input_features = input_sequences.features\n",
    "    frame_sequence_lengths = sequence_lengths.frame_sequence_lengths\n",
    "\n",
    "    # Encoder\n",
    "    encoder_params = {\n",
    "        \"feat_dim\": params['feat_dim'],\n",
    "        \"nb_classes\": params['encoder_nb_classes'],\n",
    "        \"max_nb_frames\": params['max_nb_frames'],\n",
    "        # Model parameters\n",
    "        \"nb_hidden_units\": params['encoder_nb_hidden_units'],\n",
    "        \"cell_type\": params['encoder_cell_type'],\n",
    "        \"is_bidirectional\": params['encoder_is_bidirectional'],\n",
    "        \"activation\": params['encoder_activation'],\n",
    "        \"initializer\": params['encoder_initializer'],\n",
    "        \"init_scale\": params['encoder_init_scale'],\n",
    "        \"clip_gradients\": params['encoder_clip_gradients'],\n",
    "        \"nb_layers\": params['encoder_nb_layers'],\n",
    "        \"seed\": params['seed'],\n",
    "        \"log_dir\": params['log_dir'],\n",
    "        \"dropout_rate\": params['dropout_rate']\n",
    "    }\n",
    "\n",
    "    encoder_output = rnn_encoder(\n",
    "        input_sequences=input_features, is_training=is_training,\n",
    "        sequence_lengths=frame_sequence_lengths,\n",
    "        encoder_params=encoder_params)\n",
    "\n",
    "    # Encoder Time Distributed Fully Connected Layer\n",
    "    # for segmentation task\n",
    "    frame_logits, frame_predictions, frame_y_pred = output_fc_layer(\n",
    "        encoder_output.outputs, initializer=None)\n",
    "\n",
    "    return InferenceOutput(\n",
    "        frame_logits=frame_logits,\n",
    "        frame_predictions=frame_predictions,\n",
    "        frame_y_pred=frame_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4c) Now we will define our loss and training operation (These ops are only used during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loss(logits, labels, sample_weights=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: encoder output logits\n",
    "                (3D Tensor [batch_size, max_len, nb_classes])\n",
    "        labels: ground truth frame level labels in one-hot encoding\n",
    "                [batch_size, max_len, nb_classes]\n",
    "        sample_weights: 2D Tensor [batch_size, max_len] with\n",
    "                        0 for all timesteps that are padded.\n",
    "    Returns:\n",
    "        loss_op: masked cross-entropy loss op\n",
    "    \"\"\"\n",
    "    # [batch_size, max_nb_frames, nb_classes]\n",
    "    frame_logits = logits.frame_logits\n",
    "    # [batch_size, max_nb_frames, nb_classes]\n",
    "    frame_labels = labels.frame_labels\n",
    "    # [batch_size, max_nb_frames]\n",
    "    frame_sample_weights = sample_weights.frame_sample_weights\n",
    "    \n",
    "    with tf.name_scope(\"cross_entropy_sequence_loss\"):\n",
    "        # Reshape logits, labels, sample_weights\n",
    "        # [batch_size, max_len, nb_classes] ->\n",
    "        # [batch_size*max_len, nb_classes]\n",
    "        tf.Print(frame_logits, [tf.shape(frame_logits)], \"Logits shape in loss\")\n",
    "        tf.Print(frame_labels, [tf.shape(frame_labels)], \"Labels shape in loss\")\n",
    "        nb_classes = array_ops.shape(frame_logits)[2]\n",
    "        flat_frame_logits = array_ops.reshape(frame_logits, [-1, nb_classes])\n",
    "        flat_frame_labels = array_ops.reshape(frame_labels, [-1, nb_classes])\n",
    "        # [batch_size, max_len] -> [batch_size*max_len]\n",
    "        # Compute cross-entropy for each frame separately\n",
    "        # flat_xent: 1D Tensor (batch_size*max_len,)\n",
    "        flat_frame_sample_weights = array_ops.reshape(\n",
    "            frame_sample_weights, [-1])\n",
    "        flat_frame_xent = nn_ops.softmax_cross_entropy_with_logits(\n",
    "            labels=flat_frame_labels, logits=flat_frame_logits)\n",
    "        # Apply masking on cent, setting to zero summands of\n",
    "        # the cross-entropy which correspond to padded frames\n",
    "        weighted_frame_xent = tf.multiply(flat_frame_xent, \n",
    "                                          flat_frame_sample_weights)\n",
    "        # Compute average cross-entropy loss over batches and non-padded\n",
    "        # timesteps\n",
    "        frame_xent = math_ops.reduce_sum(weighted_frame_xent)\n",
    "        nb_unpadded_frames = math_ops.reduce_sum(flat_frame_sample_weights)\n",
    "        nb_unpadded_frames += 1e-12  # avoid division by 0 for all-0 weights\n",
    "        frame_xent /= nb_unpadded_frames\n",
    "        frame_loss_op = frame_xent\n",
    "        tf.summary.scalar('frame_loss', frame_loss_op)\n",
    "\n",
    "    return frame_loss_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _grad_vars():\n",
    "    g_vars = tf.trainable_variables()\n",
    "    return g_vars\n",
    "\n",
    "\n",
    "def _train_op(loss):\n",
    "    # Add a scalar summary for loss.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    if params['decay_rate'] > 0:\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            learning_rate=params['learning_rate'],\n",
    "            global_step=global_step,\n",
    "            decay_steps=params['decay_period'],\n",
    "            decay_rate=params['decay_rate'], staircase=True)\n",
    "    else:\n",
    "        learning_rate = params['learning_rate']\n",
    "    # Create optimizer\n",
    "    optimizer_name = params['optimizer_name']\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = tf.train.MomentumOptimizer(\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=params['momentum'],\n",
    "            use_nesterov=params['nesterov'])\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'Adagrad':\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'Adadelta':\n",
    "        optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Not supported optimizer: %s\", optimizer_name)\n",
    "\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # and to increment the global step counter as a single training step.\n",
    "    # train_op = optimizer.minimize(loss, global_step=global_step,\n",
    "    # name=name)\n",
    "    g_vars = _grad_vars()\n",
    "\n",
    "    # Compute gradients\n",
    "    grads_and_vars = optimizer.compute_gradients(\n",
    "        loss, var_list=g_vars)\n",
    "\n",
    "    vars_with_grad = [v for g, v in grads_and_vars if g is not None]\n",
    "    print(\"vars_with_grad: \", vars_with_grad)\n",
    "    if not vars_with_grad:\n",
    "        raise ValueError(\n",
    "            \"No gradients provided for any variable, \"\n",
    "            \"check your graph for ops \"\n",
    "            \"that do not support gradients, \"\n",
    "            \"between variables %s and loss %s.\" %\n",
    "            ([str(v) for _, v in grads_and_vars], loss))\n",
    "\n",
    "    # Optionally clip gradients by global norm.\n",
    "    if params['encoder_clip_gradients'] is not None:\n",
    "        clipped_grads_and_vars = clip_gradients(\n",
    "            grads_and_vars, params['encoder_clip_gradients'])\n",
    "    else:\n",
    "        clipped_grads_and_vars = grads_and_vars\n",
    "\n",
    "    # Add histograms for variables, gradients and gradient norms\n",
    "    for gradient, variable in clipped_grads_and_vars:\n",
    "        if isinstance(gradient, ops.IndexedSlices):\n",
    "            grad_values = gradient.values\n",
    "        else:\n",
    "            grad_values = gradient\n",
    "\n",
    "        if grad_values is not None:\n",
    "            var_name = variable.name.replace(\":\", \"_\")\n",
    "            tf.summary.histogram(\"gradients/%s\" % var_name,\n",
    "                                 grad_values)\n",
    "            tf.summary.scalar(\"gradient_norm/%s\" % var_name,\n",
    "                              clip_ops.global_norm([grad_values]))\n",
    "\n",
    "    if params['encoder_clip_gradients'] is not None:\n",
    "        for gradient, variable in grads_and_vars:\n",
    "            if isinstance(gradient, ops.IndexedSlices):\n",
    "                grad_values = gradient.values\n",
    "            else:\n",
    "                grad_values = gradient\n",
    "\n",
    "            if grad_values is not None:\n",
    "                var_name = variable.name.replace(\":\", \"_\")\n",
    "                tf.summary.histogram(\"gradients_before_clip/%s\" % var_name,\n",
    "                                     grad_values)\n",
    "                tf.summary.scalar(\"gradient_norm_before_clip/%s\" % var_name,\n",
    "                                  clip_ops.global_norm([grad_values]))\n",
    "\n",
    "    # Create gradient updates.\n",
    "    train_op = optimizer.apply_gradients(clipped_grads_and_vars,\n",
    "                                         global_step=global_step,\n",
    "                                         name=\"train\")\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4d) Finally, we will declare placeholders for feeding data to our graph during training/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _placeholder_inputs():\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "    These placeholders are used as inputs for the model.\n",
    "    Returns: dictionary of placeholders with keys/values:\n",
    "      'features_pl': Sequences placeholder.\n",
    "      'frame_labels_pl': Frame labels placeholder.\n",
    "      'frame_sample_weights_pl': Frame sample weight placeholder\n",
    "      'frame_sequence_lengths_pl': Frame sequence lengths placeholder\n",
    "    \"\"\"\n",
    "\n",
    "    features_placeholder = tf.placeholder(\n",
    "        tf.float32,\n",
    "        [None, params['max_nb_frames'], params['feat_dim']],\n",
    "        name=\"features_pl\")\n",
    "\n",
    "    frame_labels_placeholder = tf.placeholder(\n",
    "        tf.int32,\n",
    "        [None, params['max_nb_frames'],\n",
    "         params['encoder_nb_classes']],\n",
    "        name=\"frame_labels_pl\")\n",
    "    \n",
    "    frame_sample_weights = tf.placeholder(\n",
    "        tf.float32,\n",
    "        [None, params['max_nb_frames']],\n",
    "        name=\"frame_sample_weights_pl\")\n",
    "    \n",
    "    frame_sequence_lengths_placeholder = tf.placeholder(\n",
    "        tf.int32,\n",
    "        [None],\n",
    "        name=\"frame_sequence_lengths_pl\")\n",
    "\n",
    "    is_training_placeholder = tf.placeholder(\n",
    "        tf.bool, [],\n",
    "        name=\"is_training_pl\")\n",
    "\n",
    "    placeholders = {\n",
    "        'features_pl': features_placeholder,\n",
    "        'frame_labels_pl': frame_labels_placeholder,\n",
    "        'frame_sample_weights_pl': frame_sample_weights,\n",
    "        'frame_sequence_lengths_pl':\n",
    "            frame_sequence_lengths_placeholder,\n",
    "        'is_training_pl':\n",
    "            is_training_placeholder,\n",
    "    }\n",
    "    return placeholders\n",
    "\n",
    "\n",
    "def _fill_feed_dict(generator, placeholders_dict, is_training):\n",
    "    \"\"\"Fills the feed_dict for training the given step.\n",
    "    A feed_dict takes the form of:\n",
    "    feed_dict = {\n",
    "        <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "        ....\n",
    "    }\n",
    "    Args:\n",
    "        generator: instance of SequenceBatchGenerator class\n",
    "                   use methods .__next__() for getting next batch from\n",
    "                   data and .get_all_samples() for getting\n",
    "                   all samples\n",
    "        placeholders_dict: dictionary with keys corresponding to placeholder\n",
    "         names and values consisting of the actual placeholders\n",
    "    Returns:\n",
    "      feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "      values_dict: The same dictionary but with strings as keys and\n",
    "                   not placeholders\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the feed_dict for the placeholders filled with the next\n",
    "    # `batch size` examples.\n",
    "    features, frame_labels, frame_sample_weights, \\\n",
    "        frame_sequence_lengths = \\\n",
    "        generator.__next__()\n",
    "    feed_dict = {\n",
    "        placeholders_dict['features_pl']: features,\n",
    "        placeholders_dict['frame_labels_pl']: frame_labels,\n",
    "        placeholders_dict[\n",
    "            'frame_sample_weights_pl']: frame_sample_weights,\n",
    "        placeholders_dict[\n",
    "            'frame_sequence_lengths_pl']: frame_sequence_lengths,\n",
    "        placeholders_dict['is_training_pl']: is_training,\n",
    "    }\n",
    "    values_dict = {\n",
    "        'features': features,\n",
    "        'frame_labels': frame_labels,\n",
    "        'frame_sample_weights': frame_sample_weights,\n",
    "        'frame_sequence_lengths': frame_sequence_lengths,\n",
    "        'is_training': is_training,\n",
    "    }\n",
    "\n",
    "\n",
    "    return feed_dict, values_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Functions for launching graph and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building, launching and executing operations on the graph\n",
    "def _do_train_eval(sess, train_op, loss, eval_metric, predictions,\n",
    "                   y_pred, training_generator, validation_generator,\n",
    "                   placeholders_dict, summary, summary_writer, logits):\n",
    "    \"\"\"Performs training\n",
    "    Args:\n",
    "      sess: The session in which the model has been initialized\n",
    "      train_op: Training operation\n",
    "      loss: namedtuple (\"frame_loss\")\n",
    "      eval_metric: namedtuple (\"frame_eval_metric\")\n",
    "      predictions: namedtuple (\"frame_predictions\")\n",
    "      y_pred: namedtuple (\"frame_y_pred\")\n",
    "      training_generator: instance of class SequenceBatchGenerator\n",
    "      validation_generator: instance of class SequenceBatchGenerator\n",
    "      placeholders_dict: dictionary of placeholders\n",
    "      summary_writer, summary\n",
    "      logits: namedtuple (\"frame_logits\")\n",
    "    Returns:\n",
    "        avg_metric: average eval metric evaluated over all validation\n",
    "                    sequences (\"frame_avg_metric\")\n",
    "        train_loss: average validation loss over all validation sequences\n",
    "                  (\"train_frame_loss\")\n",
    "        predictions: 3D Tensor [nb_train_samples, max_len, nb_classes]\n",
    "                  (\"frame_predictions\")\n",
    "        logits: 3D Tensor [nb_train_samples, max_len, nb_classes]\n",
    "    \"\"\"\n",
    "\n",
    "    frame_loss = loss.frame_loss\n",
    "    frame_eval_metric = eval_metric.frame_eval_metric\n",
    "    frame_predictions = predictions.frame_predictions\n",
    "    frame_logits = logits.frame_logits\n",
    "    frame_y_pred = y_pred.frame_y_pred\n",
    "\n",
    "    # Start the training loop.\n",
    "    nb_samples = training_generator.__len__()\n",
    "    nb_steps = training_generator.steps\n",
    "    logging.info('Nb of mini-batches per epoch: %d', nb_steps)\n",
    "\n",
    "    # Train\n",
    "    epoch = 0\n",
    "    avg_frame_metric = 0\n",
    "    train_frame_loss = 0\n",
    "    train_frame_loss_sum = 0\n",
    "    train_frame_y_true = []\n",
    "    train_frame_y_pred = []\n",
    "    train_frame_sample_weights = []\n",
    "\n",
    "    val_frame_res = None\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    optimization_log = defaultdict(list)\n",
    "    for iteration in range(params['nb_epochs'] * nb_steps):\n",
    "        logging.debug(\"Iteration: %d\", iteration)\n",
    "        if iteration % nb_steps == 0:\n",
    "            epoch += 1\n",
    "\n",
    "            train_frame_loss_sum = 0\n",
    "            train_frame_y_true = []\n",
    "            train_frame_y_pred = []\n",
    "            train_frame_sample_weights = []\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "        # Fill a feed dictionary with current batch of\n",
    "        # training this particular training step.\n",
    "        training_feed_dict, training_values_dict = _fill_feed_dict(\n",
    "            training_generator, placeholders_dict, is_training=1)\n",
    "\n",
    "        # Run one step of the model.  The return values are the\n",
    "        # activations from the `train_op` (which is discarded)\n",
    "        # and the `loss` Op.  To inspect the values of your Ops or\n",
    "        # variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "\n",
    "        # Train with one batch and evaluate\n",
    "        sess_run_ops = [train_op, frame_loss,\n",
    "                        frame_predictions, frame_logits, frame_y_pred]\n",
    "        sess_run_res = sess.run(sess_run_ops, feed_dict=training_feed_dict)\n",
    "        train_op_res, train_frame_loss_batch, \\\n",
    "            train_frame_predictions_batch, train_frame_logits_batch, \\\n",
    "            train_frame_y_pred_batch, \\\n",
    "            = sess_run_res\n",
    "\n",
    "        train_frame_loss_sum += train_frame_loss_batch\n",
    "\n",
    "        train_frame_y_pred.append(train_frame_y_pred_batch)\n",
    "        train_frame_y_true.append(argmax(\n",
    "            training_values_dict['frame_labels'], axis=-1))\n",
    "        train_frame_sample_weights.append(\n",
    "            training_values_dict['frame_sample_weights'])\n",
    "\n",
    "        # Write the summaries, print an overview and evaluate the model\n",
    "        # at the end of every epoch\n",
    "        if (iteration + 1) % nb_steps == 0:\n",
    "            duration = time.time() - start_time\n",
    "            # Results aggregation over all batches\n",
    "            train_frame_loss = float(train_frame_loss_sum) / nb_steps\n",
    "\n",
    "            # train_frame_y_pred: (nb_samples, nb_timesteps, nb_classes)\n",
    "            train_frame_y_pred = vstack(train_frame_y_pred)\n",
    "            # train_frame_y_true: (nb_samples, nb_timesteps, nb_classes)\n",
    "            train_frame_y_true = vstack(train_frame_y_true)\n",
    "            train_frame_sample_weights = vstack(train_frame_sample_weights)\n",
    "            avg_frame_metric = frame_eval_metric(\n",
    "                train_frame_y_true, train_frame_y_pred,\n",
    "                sample_weights=train_frame_sample_weights)\n",
    "\n",
    "            logging.info('Epoch %d: , '\n",
    "                         'nb_train_samples: %d, train_frame_metric: %f, '\n",
    "                         'train_frame_loss: %f, '\n",
    "                         'duration: %f',\n",
    "                         epoch,  nb_samples,\n",
    "                         avg_frame_metric,\n",
    "                         train_frame_loss,\n",
    "                         duration)\n",
    "\n",
    "            optimization_log['train_frame_loss'].append(train_frame_loss)\n",
    "            optimization_log['train_frame_metric'].append(avg_frame_metric)\n",
    "\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary,\n",
    "                                   feed_dict=training_feed_dict)\n",
    "            summary_writer.add_summary(summary_str, iteration)\n",
    "            summary_writer.flush()\n",
    "\n",
    "            val_frame_res = _do_eval(\n",
    "                sess=sess, loss=loss, eval_metric=eval_metric,\n",
    "                predictions=predictions, y_pred=y_pred,\n",
    "                generator=validation_generator,\n",
    "                placeholders_dict=placeholders_dict,\n",
    "                logits=logits)\n",
    "\n",
    "            optimization_log['val_frame_loss'].append(\n",
    "                val_frame_res.val_frame_loss)\n",
    "            optimization_log['val_frame_metric'].append(\n",
    "                val_frame_res.val_frame_metric)\n",
    "\n",
    "            # TODO: Log output using output_logger if available\n",
    "\n",
    "    train_frame_res = TrainFrameResults(\n",
    "        train_frame_metric=avg_frame_metric,\n",
    "        train_frame_loss=train_frame_loss,\n",
    "        train_frame_predictions=None,\n",
    "        train_frame_y_pred=train_frame_y_pred,\n",
    "        train_frame_y_true=train_frame_y_true,\n",
    "        train_frame_sample_weights=train_frame_sample_weights,\n",
    "        train_frame_logits=None,\n",
    "    )\n",
    "\n",
    "    plot_optimization_log_frame(optimization_log,\n",
    "                                params['log_dir'])\n",
    "    save_to_pickle(os.path.join(params['log_dir'], 'optimization_log.p'),\n",
    "                   optimization_log)\n",
    "\n",
    "    return train_frame_res, val_frame_res\n",
    "\n",
    "\n",
    "def _do_eval(sess, loss, eval_metric, predictions, y_pred,\n",
    "             generator, placeholders_dict, logits):\n",
    "    \"\"\"Runs one evaluation against all data samples, batch by batch.\n",
    "    Args:\n",
    "      sess: The session in which the model has been trained.\n",
    "      loss: namedtuple (\"frame_loss\")\n",
    "      eval_metric: namedtuple (\"frame_eval_metric\")\n",
    "      predictions: namedtuple (\"frame_predictions\")\n",
    "      y_pred: namedtuple (\"frame_y_pred\")\n",
    "      generator: instance of class SequenceBatchGenerator\n",
    "      placeholders_dict: dictionary of placeholders\n",
    "      logits: namedtuple (\"frame_logits\")\n",
    "    Returns:\n",
    "        avg_metric: average eval metric evaluated over all validation\n",
    "                    sequences (\"frame_avg_metric\")\n",
    "        val_loss: average validation loss over all validation sequences\n",
    "                  (\"val_frame_loss\")\n",
    "        predictions: 3D Tensor [nb_val_samples, max_len, nb_classes]\n",
    "                  (\"frame_predictions\")\n",
    "        y_pred_val: 3D Tensor [nb_val_samples, max_len]\n",
    "                  (\"frame_y_pred_val\")\n",
    "        y_true_val: 3D Tensor [nb_val_samples, max_len]\n",
    "                  (\"frame_y_true_val\")\n",
    "    \"\"\"\n",
    "\n",
    "    frame_loss = loss.frame_loss\n",
    "    frame_eval_metric = eval_metric.frame_eval_metric\n",
    "    frame_predictions = predictions.frame_predictions\n",
    "    frame_logits = logits.frame_logits\n",
    "    frame_y_pred = y_pred.frame_y_pred\n",
    "\n",
    "    # nb_samples: number of samples to evaluate against\n",
    "    # nb_steps: number of steps until we see all nb_samples\n",
    "    nb_samples = generator.__len__()\n",
    "    nb_steps = generator.steps\n",
    "\n",
    "    # And run one epoch of eval.\n",
    "\n",
    "    val_frame_loss_sum = 0\n",
    "    val_frame_predictions = []\n",
    "    val_frame_logits = []\n",
    "    val_frame_y_pred = []\n",
    "    val_frame_y_true = []\n",
    "    val_frame_sample_weights = []\n",
    "\n",
    "    for step in range(nb_steps):\n",
    "\n",
    "        feed_dict, values_dict = _fill_feed_dict(\n",
    "            generator, placeholders_dict, is_training=0)\n",
    "\n",
    "        # Frame segmentation evaluation\n",
    "        val_frame_loss_batch, \\\n",
    "            val_frame_predictions_batch, val_frame_logits_batch, \\\n",
    "            val_frame_y_pred_batch = sess.run(\n",
    "                [frame_loss, frame_predictions, frame_logits, frame_y_pred],\n",
    "                feed_dict=feed_dict)\n",
    "\n",
    "        val_frame_loss_sum += val_frame_loss_batch\n",
    "        val_frame_predictions.append(array(val_frame_predictions_batch))\n",
    "        val_frame_logits.append(array(val_frame_logits_batch))\n",
    "        val_frame_y_pred.append(val_frame_y_pred_batch)\n",
    "        val_frame_y_true.append(argmax(values_dict['frame_labels'], axis=-1))\n",
    "        val_frame_sample_weights.append(values_dict['frame_sample_weights'])\n",
    "\n",
    "    # Results aggregation over all batches\n",
    "    val_frame_loss = float(val_frame_loss_sum) / nb_steps\n",
    "    val_frame_predictions = vstack(val_frame_predictions)\n",
    "    val_frame_logits = vstack(val_frame_logits)\n",
    "    val_frame_y_pred = vstack(val_frame_y_pred)\n",
    "    val_frame_y_true = vstack(val_frame_y_true)\n",
    "    val_frame_sample_weights = vstack(val_frame_sample_weights)\n",
    "\n",
    "    avg_frame_metric = frame_eval_metric(\n",
    "        val_frame_y_true, val_frame_y_pred,\n",
    "        sample_weights=val_frame_sample_weights)\n",
    "\n",
    "    logging.info('nb_val_samples: %d, val_frame_metric: %f, '\n",
    "                 'val_frame_loss: %f',\n",
    "                 nb_samples, avg_frame_metric,\n",
    "                 val_frame_loss)\n",
    "\n",
    "    frame_res = ValFrameResults(\n",
    "        val_frame_metric=avg_frame_metric,\n",
    "        val_frame_loss=val_frame_loss,\n",
    "        val_frame_predictions=val_frame_predictions,\n",
    "        val_frame_y_pred=val_frame_y_pred,\n",
    "        val_frame_y_true=val_frame_y_true,\n",
    "        val_frame_sample_weights=val_frame_sample_weights,\n",
    "        val_frame_logits=val_frame_logits,\n",
    "    )\n",
    "\n",
    "    return frame_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_generator(training_generator, validation_generator=None):\n",
    "    \"\"\"Fit SupSeq2SeqTF Model to training_data and validate\n",
    "    on validation data\n",
    "    Args:\n",
    "        training_generator: training data generator.\n",
    "        instance of class SequenceBatchGenerator,\n",
    "        instance of class SequenceBatchGenerator,\n",
    "         or any class implementing methods: __next__() getting next batch\n",
    "          (X, y, sample_weights, sequence_lengths), get_all_samples()\n",
    "          getting all samples (X, y, sample_weights, sequence_lengths)\n",
    "          , __len__(): get number of samples, steps: get number of steps\n",
    "          required to get all samples if we load them in a batch fashion\n",
    "          and batch_size: batch size\n",
    "        validation_generator: validation data generator\n",
    "    \"\"\"\n",
    "\n",
    "    nb_steps = training_generator.steps\n",
    "    # Convert decay_rate from epochs to iterations\n",
    "    params['decay_period'] *= nb_steps\n",
    "\n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    with tf.Graph().as_default():\n",
    "        tf.set_random_seed(params['seed'])\n",
    "\n",
    "        # Generate placeholders for training/validation data\n",
    "        placeholders_dict = _placeholder_inputs()\n",
    "\n",
    "        # Build a Graph that computes predictions from the inference model.\n",
    "        input_sequences_tuple = namedtuple(\"InputSequences\",\n",
    "                                           \"features\")\n",
    "        input_sequences = input_sequences_tuple(\n",
    "            features=placeholders_dict['features_pl']\n",
    "        )\n",
    "        sequence_lengths_tuple = namedtuple(\n",
    "            \"SequenceLengths\",\n",
    "            \"frame_sequence_lengths\")\n",
    "        sequence_lengths = sequence_lengths_tuple(\n",
    "            frame_sequence_lengths=placeholders_dict[\n",
    "                'frame_sequence_lengths_pl'],\n",
    "        )\n",
    "\n",
    "        inference_output = _inference(\n",
    "            input_sequences=input_sequences,\n",
    "            is_training=placeholders_dict['is_training_pl'],\n",
    "            sequence_lengths=sequence_lengths)\n",
    "\n",
    "        # Parse inference output\n",
    "        frame_logits = inference_output.frame_logits\n",
    "        frame_predictions = inference_output.frame_predictions\n",
    "        frame_y_pred = inference_output.frame_y_pred\n",
    "\n",
    "        # Create inputs for _loss and _do_train_eval functions\n",
    "        logits_tuple = namedtuple(\"logits\",\n",
    "                                  \"frame_logits\")\n",
    "        logits = logits_tuple(\n",
    "            frame_logits=frame_logits,\n",
    "        )\n",
    "        labels_tuple = namedtuple(\"labels\",\n",
    "                                  \"frame_labels\")\n",
    "        labels = labels_tuple(\n",
    "            frame_labels=placeholders_dict['frame_labels_pl'],\n",
    "        )\n",
    "        sample_weights_tuple = namedtuple(\n",
    "            \"sample_weights\",\n",
    "            \"frame_sample_weights\")\n",
    "        sample_weights = sample_weights_tuple(\n",
    "            frame_sample_weights=placeholders_dict[\n",
    "                'frame_sample_weights_pl'],\n",
    "        )\n",
    "\n",
    "        # Add to the Graph the Ops for loss calculation.\n",
    "        frame_loss = _loss(\n",
    "            logits=logits, labels=labels,\n",
    "            sample_weights=sample_weights\n",
    "        )\n",
    "\n",
    "        grad_vars = _grad_vars()\n",
    "        # Add to the Graph the Ops that calculate and apply gradients.\n",
    "        train_op = _train_op(frame_loss)\n",
    "\n",
    "        # Add the Op to compare predictions to labels during evaluation.\n",
    "        loss_tuple = namedtuple(\"loss\",\n",
    "                                \"frame_loss\")\n",
    "        loss = loss_tuple(\n",
    "            frame_loss=frame_loss,\n",
    "        )\n",
    "        eval_metric_tuple = namedtuple(\"eval_metric\",\n",
    "                                       \"frame_eval_metric\")\n",
    "        eval_metric = eval_metric_tuple(\n",
    "            frame_eval_metric=per_frame_accuracy,\n",
    "        )\n",
    "        predictions_tuple = namedtuple(\"predictions\",\n",
    "                                       \"frame_predictions\")\n",
    "        predictions = predictions_tuple(\n",
    "            frame_predictions=frame_predictions,\n",
    "        )\n",
    "        y_pred_tuple = namedtuple(\"y_pred\",\n",
    "                                  \"frame_y_pred\")\n",
    "        y_pred = y_pred_tuple(\n",
    "            frame_y_pred=frame_y_pred,\n",
    "        )\n",
    "\n",
    "        # Build the summary Tensor based on the TF collection of Summaries.\n",
    "        summary = tf.summary.merge_all()\n",
    "\n",
    "        # Add the variable initializer Op.\n",
    "        init = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "\n",
    "        # Create a saver for writing training checkpoints.\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Create a session for running Ops on the Graph.\n",
    "        sess = tf.Session()\n",
    "\n",
    "        # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "        summary_writer = tf.summary.FileWriter(params['log_dir'],\n",
    "                                               sess.graph)\n",
    "\n",
    "        # And then after everything is built:\n",
    "        # Run the Op to initialize the variables.\n",
    "        sess.run(init)\n",
    "        sess.run(init_l)\n",
    "\n",
    "        # Print number of trainable parameters\n",
    "        nb_trainable_params = sum([prod(v.get_shape().as_list())\n",
    "                                   for v in grad_vars])\n",
    "        logging.info(\"Number of trainable params: %d\", nb_trainable_params)\n",
    "        # Train and evaluate at each epoch\n",
    "        nb_training_steps_per_epoch = training_generator.steps\n",
    "        train_frame_res, val_frame_res = \\\n",
    "            _do_train_eval(\n",
    "                sess=sess, train_op=train_op, loss=loss,\n",
    "                eval_metric=eval_metric, predictions=predictions,\n",
    "                y_pred=y_pred,\n",
    "                training_generator=training_generator,\n",
    "                validation_generator=validation_generator,\n",
    "                placeholders_dict=placeholders_dict,\n",
    "                summary=summary, summary_writer=summary_writer,\n",
    "                logits=logits)\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint_file = os.path.join(params['log_dir'],\n",
    "                                       'multilabel_rnn_model.ckpt')\n",
    "        nb_total_steps = params[\n",
    "                             'nb_epochs'] * nb_training_steps_per_epoch\n",
    "        saver.save(sess, checkpoint_file,\n",
    "                   global_step=nb_total_steps - 1)\n",
    "\n",
    "        # TODO: Log output using output_logger if available\n",
    "\n",
    "        return val_frame_res, train_frame_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_frame_res, train_frame_res = fit_generator(training_generator=training_batch_generator_obj, validation_generator=testing_batch_generator_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
